{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656de0ec-f9bf-4303-bcb7-5413c07b801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 12:33:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/17 12:33:00 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.\n",
      "22/05/17 12:33:03 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state TERMINATED, waiting for it to start running...\n",
      "22/05/17 12:33:13 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:33:23 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:33:33 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:33:44 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:33:54 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:34:04 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:34:14 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:34:24 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:34:34 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:34:44 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:34:54 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:35:04 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:35:14 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:35:25 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:35:35 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:35:45 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:35:55 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:36:05 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:36:15 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:36:25 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:36:35 WARN SparkServiceRPCClient: Cluster 0427-122644-45iadnd in state PENDING, waiting for it to start running...\n",
      "22/05/17 12:37:03 WARN DBFS: DBFS listStatus on /mnt took 1136 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FileInfo(path='dbfs:/mnt/DLQ/', name='DLQ/', size=0, modificationTime=0), FileInfo(path='dbfs:/mnt/SPT/', name='SPT/', size=0, modificationTime=0), FileInfo(path='dbfs:/mnt/adls/', name='adls/', size=0, modificationTime=0), FileInfo(path='dbfs:/mnt/azuwevelbwdls01q/', name='azuwevelbwdls01q/', size=0, modificationTime=0), FileInfo(path='dbfs:/mnt/conformed/', name='conformed/', size=0, modificationTime=0), FileInfo(path='dbfs:/mnt/delta/', name='delta/', size=0, modificationTime=1611768799000), FileInfo(path='dbfs:/mnt/raw/', name='raw/', size=0, modificationTime=0)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display\n",
    "from pyspark.dbutils import DBUtils\n",
    "from datetime import date\n",
    "import os\n",
    "\n",
    "# Testing reading directory\n",
    "\n",
    "spark = SparkSession.builder.appName('viadot').getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "response = dbutils.fs.ls(\"/mnt/\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb26af4f-96c9-4f8b-a632-54088dcfd304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-17 13:51:28+0000] INFO - prefect.test | Waiting for next scheduled run at 2022-05-17T13:52:00+00:00\n",
      "[2022-05-17 13:52:00+0000] INFO - prefect.FlowRunner | Beginning Flow run for 'test'\n",
      "[2022-05-17 13:52:00+0000] INFO - prefect.TaskRunner | Task 'generate_fake_data': Starting task run...\n",
      "[2022-05-17 13:52:00+0000] INFO - prefect.TaskRunner | Task 'generate_fake_data': Finished task run for task with final state: 'Success'\n",
      "[2022-05-17 13:52:00+0000] INFO - prefect.TaskRunner | Task 'write_to_spark': Starting task run...\n",
      "View job details at https://adb-1930462786844525.5.azuredatabricks.net/?o=1930462786844525#/setting/clusters/0427-122644-45iadnd/sparkUi\n",
      "[2022-05-17 13:52:02+0000] INFO - prefect.TaskRunner | Task 'write_to_spark': Finished task run for task with final state: 'Success'\n",
      "[2022-05-17 13:52:02+0000] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded\n",
      "[2022-05-17 13:52:02+0000] INFO - prefect.test | Waiting for next scheduled run at 2022-05-17T13:53:00+00:00\n",
      "[2022-05-17 13:53:00+0000] INFO - prefect.FlowRunner | Beginning Flow run for 'test'\n",
      "[2022-05-17 13:53:00+0000] INFO - prefect.TaskRunner | Task 'generate_fake_data': Starting task run...\n",
      "[2022-05-17 13:53:00+0000] INFO - prefect.TaskRunner | Task 'generate_fake_data': Finished task run for task with final state: 'Success'\n",
      "[2022-05-17 13:53:00+0000] INFO - prefect.TaskRunner | Task 'write_to_spark': Starting task run...\n",
      "View job details at https://adb-1930462786844525.5.azuredatabricks.net/?o=1930462786844525#/setting/clusters/0427-122644-45iadnd/sparkUi\n",
      "[2022-05-17 13:53:02+0000] INFO - prefect.TaskRunner | Task 'write_to_spark': Finished task run for task with final state: 'Success'\n",
      "[2022-05-17 13:53:02+0000] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded\n",
      "[2022-05-17 13:53:02+0000] INFO - prefect.test | Waiting for next scheduled run at 2022-05-17T13:54:00+00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     write_to_spark(df)\n\u001b[1;32m     23\u001b[0m f\u001b[38;5;241m.\u001b[39mvisualize()\n\u001b[0;32m---> 24\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/prefect/core/flow.py:1274\u001b[0m, in \u001b[0;36mFlow.run\u001b[0;34m(self, parameters, run_on_schedule, runner_cls, **kwargs)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_on_schedule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m     run_on_schedule \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mbool\u001b[39m, prefect\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mflows\u001b[38;5;241m.\u001b[39mrun_on_schedule)\n\u001b[0;32m-> 1274\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunner_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrunner_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_on_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_on_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# state always should return a dict of tasks. If it's empty (meaning the run was\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m# interrupted before any tasks were executed), we set the dict manually.\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state\u001b[38;5;241m.\u001b[39m_result:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/prefect/core/flow.py:1079\u001b[0m, in \u001b[0;36mFlow._run\u001b[0;34m(self, parameters, runner_cls, run_on_schedule, **kwargs)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m naptime \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1076\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1077\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for next scheduled run at \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(next_run_time)\n\u001b[1;32m   1078\u001b[0m         )\n\u001b[0;32m-> 1079\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnaptime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;66;03m# begin a single flow run\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from prefect import task, Flow\n",
    "from prefect.schedules import IntervalSchedule\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "\n",
    "# Flow for writing data every minute to a table\n",
    "\n",
    "# Convert list to pandas dataframe\n",
    "@task\n",
    "def generate_fake_data(data: list):\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Convert pandas dataframe to spark dataframe then write to table\n",
    "@task\n",
    "def write_to_spark(df: pd.DataFrame):\n",
    "  sparkdf = spark.createDataFrame(df)\n",
    "  sparkdf.write.mode(\"append\").saveAsTable(\"raw.c4c_test4\")\n",
    "\n",
    "# Flow scheduled for every minute\n",
    "schedule = IntervalSchedule(interval=timedelta(minutes=1))\n",
    "with Flow(\"test\", schedule=schedule) as f:\n",
    "    data_raw =[{\"Id\": \"KVSzUaILfQZXDb\" + str(datetime.datetime.now()), \"AccountId\": \"EHNYKjSZsiy\", \"Name\": \"Turner-Black\", \"FirstName\": \"Adam\", \"LastName\": \"Carter\", \"ContactEMail\": \"Adam.Carter@TurnerBlack.com\", \"MailingCity\": \"Jamesport\"}]\n",
    "    df = generate_fake_data(data_raw)\n",
    "    write_to_spark(df)\n",
    "    \n",
    "f.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ae98113-4079-4ec1-a816-bffab1ca5a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-17 13:42:54.358083\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7af8d-c075-4c78-a113-18b5afa1ebe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
